{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Merge Script\n",
    "\n",
    "This example demonstrates how to merge two checkpoints using Fisher mask nodes. For new tasks and architectures \n",
    "the code may need to be modified, but it's generally straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We use `bert-tiny` with the tasks `mnli` and `sst2` for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 11:34:36.962685: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-13 11:34:36.962723: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-13 11:34:36.963768: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-13 11:34:37.037724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-13 11:34:37.941699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from calc_fisher import calculate_fisher\n",
    "from fisher_nodes import FisherNodeWrapper\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertModel\n",
    "\n",
    "SEED = 0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_SAMPLES = 128\n",
    "\n",
    "checkpoint_names = [\"M-FAC/bert-tiny-finetuned-mnli\", \"M-FAC/bert-tiny-finetuned-sst2\"]\n",
    "tasks = [\"mnli\", \"sst2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thennal/virtualenvs/random/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_names[0], max_len=512)\n",
    "checkpoints = [AutoModelForSequenceClassification.from_pretrained(name) for name in checkpoint_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the head of the model is different for each task, we need to keep track of it seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropouts = [checkpoint.dropout for checkpoint in checkpoints]\n",
    "classifiers = [checkpoint.classifier for checkpoint in checkpoints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Fisher Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`neuron_mask` and `head_mask` provide the tensors corresponding to the Fisher information of the inserted nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3c1e54b5774c399bd3d6321e8fc49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.60it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab4c495dfd745f2aea048d7bed93e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 175.45it/s]\n"
     ]
    }
   ],
   "source": [
    "neuron_masks = []\n",
    "head_masks = []\n",
    "for task, checkpoint in zip(tasks, checkpoints):\n",
    "    neuron_mask, head_mask = calculate_fisher(\n",
    "        model=checkpoint,\n",
    "        task_name=task,\n",
    "        tokenizer=tokenizer,\n",
    "        num_samples=NUM_SAMPLES, \n",
    "        device=DEVICE, \n",
    "        seed=0)\n",
    "    \n",
    "    neuron_masks.append(neuron_mask)\n",
    "    head_masks.append(head_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FisherNodeWrapper` is an abstraction to store the Fisher information of the nodes. As it directly implements the `__add__` method, we can simply add them together to perform the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrappers = [\n",
    "    FisherNodeWrapper(finetuned=checkpoint, neuron_mask=neuron_mask, head_mask=head_mask) \n",
    "    for checkpoint, neuron_mask, head_mask in zip(checkpoints, neuron_masks, head_masks)\n",
    "    ]\n",
    "merged_params = sum(model_wrappers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply_to` sets the model parameters to that of the merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "model = merged_params.apply_to(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test with an example from the appropriate dataset. As BERT requires heads for each task, we use the appropriate head stored in `classifiers` and `dropouts`. For other architectures, this may not be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"Fun for adults and children.\"\n",
    "hypothesis = \"Fun for only children.\"\n",
    "mnli_labels = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_output = model(**inputs).pooler_output\n",
    "    logits = classifiers[0](dropouts[0](bert_output))\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "mnli_labels[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"equals the original and in some ways even betters it\"\n",
    "sst2_labels = ['negative', 'positive']\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_output = model(**inputs).pooler_output\n",
    "    logits = classifiers[1](dropouts[1](bert_output))\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "sst2_labels[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
